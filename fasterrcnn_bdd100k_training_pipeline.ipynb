{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd628133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Setup\n",
    "# !pip install torchvision --upgrade --quiet  (if required)\n",
    "\n",
    "\n",
    "import os, json, torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03cd7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Config\n",
    "CLASS_MAP = {\n",
    "    \"car\": 1,\n",
    "    \"person\": 2,\n",
    "    \"bike\": 3,\n",
    "    \"traffic light\": 4,\n",
    "    \"train\": 5,\n",
    "    \"rider\": 6,\n",
    "    \"bus\": 7,\n",
    "    \"truck\": 8,\n",
    "    \"traffic sign\": 9,\n",
    "}\n",
    "\n",
    "\n",
    "NUM_CLASSES = len(CLASS_MAP) + 1\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ca350-2057-4a3c-9927-b247aa05fc50",
   "metadata": {},
   "source": [
    "The BDDDataset class is a custom PyTorch Dataset implementation for loading and processing the BDD100K dataset, specifically for object detection tasks. This dataset class is designed to work seamlessly with models like Faster R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87eb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Dataset\n",
    "class BDDDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation_file, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        with open(annotation_file) as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.annotations[idx]\n",
    "        img_path = os.path.join(self.root, ann[\"name\"])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        boxes, labels = [], []\n",
    "        for obj in ann[\"labels\"]:\n",
    "            if \"box2d\" not in obj or obj[\"category\"] not in CLASS_MAP:\n",
    "                continue\n",
    "            b = obj[\"box2d\"]\n",
    "            boxes.append([b[\"x1\"], b[\"y1\"], b[\"x2\"], b[\"y2\"]])\n",
    "            labels.append(CLASS_MAP[obj[\"category\"]])\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "        }\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f418d-f98a-496f-82c0-221a6eb89688",
   "metadata": {},
   "source": [
    "This function calculates class weights based on the frequency of each class in the BDD100K annotations. These weights can be used to handle class imbalance during training, especially for object detection tasks.\n",
    "\n",
    "This approach ensures that rare classes get higher loss weight, improving model performance on under-represented categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7d0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Class Weights\n",
    "def compute_class_weights(annotation_file):\n",
    "    with open(annotation_file) as f:\n",
    "        data = json.load(f)\n",
    "    counter = Counter()\n",
    "    for item in data:\n",
    "        for obj in item[\"labels\"]:\n",
    "            if \"box2d\" in obj and obj[\"category\"] in CLASS_MAP:\n",
    "                counter[CLASS_MAP[obj[\"category\"]]] += 1\n",
    "    max_count = max(counter.values())\n",
    "    weights = {\n",
    "        cls: max_count / count for cls, count in counter.items()\n",
    "    }  # for count max\n",
    "    return torch.tensor([1.0] + [weights.get(i, 1.0) for i in range(1, NUM_CLASSES)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3985f4-1ac6-470f-9ede-07136155f3a7",
   "metadata": {},
   "source": [
    "This class extends PyTorchâ€™s built-in FasterRCNN model to incorporate class-weighted loss, helping tackle class imbalance during object detection training.\n",
    "\n",
    "This implementation modifies only the classification loss. we could extend it to include weighting in other loss components (like bbox regression) if needeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c85269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with weighted loss\n",
    "class WeightedFasterRCNN(FasterRCNN):\n",
    "    def __init__(self, backbone, num_classes, class_weights=None):\n",
    "        super().__init__(backbone, num_classes)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        if self.training and self.class_weights is not None:\n",
    "            losses = super().forward(images, targets)\n",
    "            for t in targets:\n",
    "                weights = self.class_weights.to(images[0].device)[t[\"labels\"]]\n",
    "                losses[\"loss_classifier\"] *= weights.mean()\n",
    "\n",
    "            return losses\n",
    "        return super().forward(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad14370-489e-485d-ae32-af85bf2954ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8dc907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    from collections import defaultdict\n",
    "    import numpy as np\n",
    "\n",
    "    aps = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images = list(i.to(device) for i in images)\n",
    "            outputs = model(images)\n",
    "            for pred, tgt in zip(outputs, targets):\n",
    "                if len(pred[\"boxes\"]) == 0 or len(tgt[\"boxes\"]) == 0:\n",
    "                    continue\n",
    "                ious = box_iou(pred[\"boxes\"].cpu(), tgt[\"boxes\"].cpu())\n",
    "                matches = (ious > 0.5).sum().item()\n",
    "                aps.append(matches / max(len(tgt[\"boxes\"]), 1))\n",
    "    return np.mean(aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "269940f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Paths (update for your dataset)\n",
    "TRAIN_IMG_DIR = (\n",
    "    \"/nfs/interns/kuldeepk/Assignment/bdd100k_images_100k/bdd100k/images/100k/train/\"\n",
    ")\n",
    "VAL_IMG_DIR = (\n",
    "    \"/nfs/interns/kuldeepk/Assignment/bdd100k_images_100k/bdd100k/images/100k/val/\"\n",
    ")\n",
    "TRAIN_JSON = \"/nfs/interns/kuldeepk/Assignment/bdd100k_labels_release/bdd100k/labels/bdd100k_labels_images_train.json\"\n",
    "VAL_JSON = \"/nfs/interns/kuldeepk/Assignment/bdd100k_labels_release/bdd100k/labels/bdd100k_labels_images_val.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5408fa-a2b0-4c7f-92bb-628d830f8acd",
   "metadata": {},
   "source": [
    "This section sets up image preprocessing, dataset loading, and dataloaders for training and validation using the BDD100K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e3e4ae-2601-4f59-9e63-f1d8c3257071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Loaders\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=(0.5, 0.5, 0.5),\n",
    "            std=(0.5, 0.5, 0.5),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_ds = BDDDataset(TRAIN_IMG_DIR, TRAIN_JSON, transform)\n",
    "val_ds = BDDDataset(VAL_IMG_DIR, VAL_JSON, transform)\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11d740-7e1d-422f-9770-fd880ce50e04",
   "metadata": {},
   "source": [
    "This section sets up a Faster R-CNN model with a ResNet-50 FPN backbone and integrates class-weighted loss to improve performance on imbalanced datasets like BDD100K.\n",
    "\n",
    "Computes inverse-frequency class weights based on training annotations.\n",
    "\n",
    "Used to penalize under-represented classes more heavily during training.\n",
    "\n",
    "Uses ResNet-50 pretrained on ImageNet (v2) as the feature extractor.\n",
    "\n",
    "Combines it with a Feature Pyramid Network (FPN) to handle multi-scale object detection.\n",
    "\n",
    "Instantiates the custom WeightedFasterRCNN model.\n",
    "\n",
    "Injects class_weights into the model to modify classification loss based on class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36786047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/interns/kuldeepk/anaconda3/envs/Yoov5/lib/python3.9/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WeightedFasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=10, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "weights = compute_class_weights(TRAIN_JSON)\n",
    "backbone = resnet_fpn_backbone(\"resnet50\", weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "model = WeightedFasterRCNN(backbone, NUM_CLASSES, weights)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626aae70-5fb1-4edd-b15f-adbc6f3db0bd",
   "metadata": {},
   "source": [
    "Sets up the AdamW optimizer to train the WeightedFasterRCNN model.\n",
    "A variant of Adam that decouples weight decay from the gradient update.\n",
    "\n",
    "Helps prevent overfitting and improves generalization compared to regular Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5c5f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba3125-fb4e-4ca3-935c-71d1fbc18e08",
   "metadata": {},
   "source": [
    "This section implements manual microbatching to simulate larger batch sizes than the GPU memory allows. Itâ€™s especially useful when using large detection models like Faster R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7dad199-e719-4b4b-a82e-8dadc5243680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_microbatches(x_batch, y_batch, micro_batch_size):\n",
    "    batch_size = len(x_batch)\n",
    "    assert (\n",
    "        batch_size % micro_batch_size == 0\n",
    "    ), f\"batch_size % mini_batch_size != 0, {batch_size = }, {micro_batch_size = }\"\n",
    "    for idx in range(0, batch_size, micro_batch_size):\n",
    "        x_micro_batch = x_batch[idx : idx + micro_batch_size]\n",
    "        y_micro_batch = y_batch[idx : idx + micro_batch_size]\n",
    "        yield x_micro_batch, y_micro_batch\n",
    "\n",
    "\n",
    "def process_microbatches(model, x_batch, y_batch, micro_batch_size, autocast):\n",
    "    batch_size = len(x_batch)\n",
    "    grad_accum_step = batch_size // micro_batch_size\n",
    "    loss_accum = 0\n",
    "    for x, y in make_microbatches(x_batch, y_batch, micro_batch_size):\n",
    "        x = [x_.to(device) for x_ in x]\n",
    "        # [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "\n",
    "        with autocast:\n",
    "            loss_dict = model(x, y)\n",
    "            loss = sum(l for l in loss_dict.values())\n",
    "            loss /= grad_accum_step\n",
    "        # loss.backward()\n",
    "        loss_accum += loss.item()\n",
    "    return loss_accum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a93296-230e-4415-a002-88ded8d39700",
   "metadata": {},
   "source": [
    "Training Loop with Microbatching, mAP Evaluation, and Checkpointing\n",
    "This section defines the training loop for a WeightedFasterRCNN model using microbatching, mixed-precision support, and periodic evaluation + checkpoint saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a635bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 : loss_accum = 6584471.5396\n",
      "step = 1 : loss_accum = 4458453.7973\n",
      "step = 2 : loss_accum = 10326817.8061\n",
      "step = 3 : loss_accum = 2498142.3282\n",
      "step = 4 : loss_accum = 847703.3582\n",
      "step = 5 : loss_accum = 5839784.9164\n",
      "step = 6 : loss_accum = 3598979.6368\n",
      "step = 7 : loss_accum = 6223663.0996\n",
      "step = 8 : loss_accum = 3499921.3249\n",
      "step = 9 : loss_accum = 2469233.9170\n",
      "step = 10 : loss_accum = 2428314.3585\n",
      "step = 11 : loss_accum = 7135705.3408\n",
      "step = 12 : loss_accum = 503034919.0657\n",
      "step = 13 : loss_accum = 8630432.8462\n",
      "step = 14 : loss_accum = 5695378.5226\n",
      "step = 15 : loss_accum = 7354514.4047\n",
      "step = 16 : loss_accum = 1878560.8758\n",
      "step = 17 : loss_accum = 2605182.8343\n",
      "step = 18 : loss_accum = 462440946.1577\n",
      "[Epoch 1] ðŸ”§ Avg Train Loss: 55134269.7963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:34<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] ðŸ“ˆ Validation mAP@0.5: 0.0055\n",
      "step = 19 : loss_accum = 7100458.8388\n",
      "step = 20 : loss_accum = 20736922.2513\n",
      "step = 21 : loss_accum = 20583103.8011\n",
      "step = 22 : loss_accum = 9266637.1589\n",
      "step = 23 : loss_accum = 2781443.0003\n",
      "step = 24 : loss_accum = 2068906.3684\n",
      "step = 25 : loss_accum = 4030338.4642\n",
      "step = 26 : loss_accum = 2396036.1604\n",
      "step = 27 : loss_accum = 11079258.2587\n",
      "step = 28 : loss_accum = 10723873.8278\n",
      "step = 29 : loss_accum = 3318067.6622\n",
      "step = 30 : loss_accum = 18032337.3308\n",
      "step = 31 : loss_accum = 3814144.2119\n",
      "step = 32 : loss_accum = 8070504.5404\n",
      "step = 33 : loss_accum = 7006673.7757\n",
      "step = 34 : loss_accum = 9167528.6405\n",
      "step = 35 : loss_accum = 5917772.3482\n",
      "step = 36 : loss_accum = 8348695.7622\n",
      "step = 37 : loss_accum = 2355892.6817\n",
      "[Epoch 2] ðŸ”§ Avg Train Loss: 8252557.6360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:37<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] ðŸ“ˆ Validation mAP@0.5: 0.0055\n",
      "step = 38 : loss_accum = 16004068.0690\n",
      "step = 39 : loss_accum = 2715678.5806\n",
      "step = 40 : loss_accum = 3417676.2224\n",
      "step = 41 : loss_accum = 2447831.3285\n",
      "step = 42 : loss_accum = 4724502.9034\n",
      "step = 43 : loss_accum = 3946771.2948\n",
      "step = 44 : loss_accum = 5100091.2750\n",
      "step = 45 : loss_accum = 17370000.5526\n",
      "step = 46 : loss_accum = 3454095.6666\n",
      "step = 47 : loss_accum = 3830411.2036\n",
      "step = 48 : loss_accum = 4918444.9487\n",
      "step = 49 : loss_accum = 6054920.4000\n",
      "step = 50 : loss_accum = 2301932.4552\n",
      "step = 51 : loss_accum = 2282747.7560\n",
      "step = 52 : loss_accum = 28408975.0327\n",
      "step = 53 : loss_accum = 7909977.1039\n",
      "step = 54 : loss_accum = 11120736.3941\n",
      "step = 55 : loss_accum = 2141946.1736\n",
      "step = 56 : loss_accum = 345790924.7464\n",
      "[Epoch 3] ðŸ”§ Avg Train Loss: 24944301.6898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:33<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] ðŸ“ˆ Validation mAP@0.5: 0.0055\n",
      "step = 57 : loss_accum = 4348856.0245\n",
      "step = 58 : loss_accum = 1768828.1896\n",
      "step = 59 : loss_accum = 11977422.8573\n",
      "step = 60 : loss_accum = 1944127.8754\n",
      "step = 61 : loss_accum = 1877966.0294\n",
      "step = 62 : loss_accum = 8943424.2892\n",
      "step = 63 : loss_accum = 2878496.5831\n",
      "step = 64 : loss_accum = 8646509.2423\n",
      "step = 65 : loss_accum = 4835932.8317\n",
      "step = 66 : loss_accum = 74346228.5115\n",
      "step = 67 : loss_accum = 6131043.1694\n",
      "step = 68 : loss_accum = 4610674.9167\n",
      "step = 69 : loss_accum = 11333281.8082\n",
      "step = 70 : loss_accum = 142327072.7335\n",
      "step = 71 : loss_accum = 4572388.5061\n",
      "step = 72 : loss_accum = 2376486.9674\n",
      "step = 73 : loss_accum = 1842075.3290\n",
      "step = 74 : loss_accum = 2268675.6728\n",
      "step = 75 : loss_accum = 13185004.2200\n",
      "[Epoch 4] ðŸ”§ Avg Train Loss: 16327078.7241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:36<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] ðŸ“ˆ Validation mAP@0.5: 0.0055\n",
      "step = 76 : loss_accum = 14802608.2071\n",
      "step = 77 : loss_accum = 2733709.8220\n",
      "step = 78 : loss_accum = 2178649.2544\n",
      "step = 79 : loss_accum = 8883601.7321\n",
      "step = 80 : loss_accum = 1749944.7011\n",
      "step = 81 : loss_accum = 29959706.6156\n",
      "step = 82 : loss_accum = 3504663.9195\n",
      "step = 83 : loss_accum = 2929445.5255\n",
      "step = 84 : loss_accum = 6860004.8596\n",
      "step = 85 : loss_accum = 4549882.4912\n",
      "step = 86 : loss_accum = 17518728.2002\n",
      "step = 87 : loss_accum = 14057708.3387\n",
      "step = 88 : loss_accum = 6464720.5658\n",
      "step = 89 : loss_accum = 9989210.2901\n",
      "step = 90 : loss_accum = 4272660.8139\n",
      "step = 91 : loss_accum = 1627533.8971\n",
      "step = 92 : loss_accum = 8604618.3347\n",
      "step = 93 : loss_accum = 4265390.4308\n",
      "step = 94 : loss_accum = 1976839.2299\n",
      "[Epoch 5] ðŸ”§ Avg Train Loss: 7733138.2752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [09:30<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] ðŸ“ˆ Validation mAP@0.5: 0.0055\n"
     ]
    }
   ],
   "source": [
    "#  Training Loop + Checkpoints\n",
    "EPOCHS = 5\n",
    "micro_batch_size = 8\n",
    "use_float16 = False\n",
    "autocast = torch.autocast(\"cuda\", torch.float16, enabled=use_float16)\n",
    "\n",
    "SAVE_PATH = \"./content/checkpoints\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, targets in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_accum = process_microbatches(\n",
    "            model, images, targets, micro_batch_size, autocast\n",
    "        )\n",
    "        optimizer.step()\n",
    "        # total_loss += loss.item()\n",
    "        total_loss += loss_accum\n",
    "        print(f\"{step = } : {loss_accum = :.4f}\")\n",
    "        step += 1\n",
    "\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"[Epoch {epoch+1}] ðŸ”§ Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # mAP evaluation\n",
    "    mAP = evaluate(model, val_dl, device)\n",
    "    print(f\"[Epoch {epoch+1}] ðŸ“ˆ Validation mAP@0.5: {mAP:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(\n",
    "        model.state_dict(), os.path.join(SAVE_PATH, f\"fasterrcnn_epoch{epoch+1}.pth\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d0e9e-8c0c-4b4f-ad47-39ed8c76fd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
